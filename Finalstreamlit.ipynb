{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS/mmAfKUOKzncrtR4Y2fx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saudaziz555/LLM-streamlit-app/blob/main/Finalstreamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZNN8IKYEhbs"
      },
      "source": [
        "# Updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AriaxZMAElwX"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Package Installation\n",
        "%%capture\n",
        "# Update package lists and install poppler-utils\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils\n",
        "\n",
        "# Install Python packages\n",
        "!pip install streamlit google-generativeai pandas pillow pdf2image tenacity\n",
        "!pip install PyMuPDF openpyxl bitsandbytes accelerate\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install pyngrok\n",
        "!pip install rapidfuzz\n",
        "!pip install unsloth_zoo"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5erGEBaaK27I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHH4mo_7Eo5H",
        "outputId": "3761c76b-d17a-415d-b7a1-ed7481bec3b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import re\n",
        "import time\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "import fitz  # PyMuPDF\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import tenacity\n",
        "import io\n",
        "import warnings\n",
        "from rapidfuzz import fuzz\n",
        "import sqlite3\n",
        "from google.colab import userdata\n",
        "import IPython\n",
        "import ipykernel\n",
        "\n",
        "\n",
        "\n",
        "# GEMINI API KEY\n",
        "try:\n",
        "    gen_key = userdata.get('GENAI_API_KEY')\n",
        "    if gen_key is None:\n",
        "        raise ValueError(\"GENAI_API_KEY not found in secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting API key: {e}\")\n",
        "\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    filename='processing_log.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    filemode='a'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Streamlit app started.\")\n",
        "\n",
        "# Base directory\n",
        "BASE_DIR = \"/content/drive/MyDrive\"\n",
        "DB_FILE = BASE_DIR+\"/results.db\"\n",
        "TABLE_NAME = \"student_results\"\n",
        "LOG_TABLE = \"process_logs\"\n",
        "\n",
        "# Initialize Gemini\n",
        "try:\n",
        "    genai.configure(api_key=gen_key)\n",
        "    gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    print(\"Gemini model initialized successfully.\") # Optional success message\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to initialize Gemini model: {str(e)}\")\n",
        "    # st.error(f\"Failed to initialize Gemini model: {str(e)}\") # Assuming 'st' is Streamlit or similar\n",
        "    gemini_model = None\n",
        "\n",
        "# Initialize LLaMA\n",
        "@st.cache_resource\n",
        "def load_llama_model():\n",
        "    try:\n",
        "        model, tokenizer = FastVisionModel.from_pretrained(\n",
        "            \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "            load_in_4bit=True,\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "        )\n",
        "        FastVisionModel.for_inference(model)\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load LLaMA model: {str(e)}\")\n",
        "        st.error(f\"Error loading LLaMA model: {e}. Consider upgrading to Colab Pro+.\")\n",
        "        return None, None\n",
        "\n",
        "llama_model, llama_tokenizer = load_llama_model()\n",
        "\n",
        "# Retry decorator for Gemini API\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=2, max=10),\n",
        "    stop=tenacity.stop_after_attempt(5),\n",
        "    retry=tenacity.retry_if_exception_message(match=\"429\")\n",
        ")\n",
        "def generate_gemini_content(contents):\n",
        "    if gemini_model is None:\n",
        "        raise Exception(\"Gemini model not initialized.\")\n",
        "    time.sleep(2)\n",
        "    return gemini_model.generate_content(contents)\n",
        "\n",
        "def load_file(file_path, zoom_x=2.0, zoom_y=2.0):\n",
        "    try:\n",
        "        if file_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img = Image.open(file_path).convert(\"RGB\")\n",
        "            return [img]\n",
        "        elif file_path.lower().endswith('.pdf'):\n",
        "            doc = fitz.open(file_path)\n",
        "            images = []\n",
        "            for page_num in range(doc.page_count):\n",
        "                page = doc.load_page(page_num)\n",
        "                mat = fitz.Matrix(zoom_x, zoom_y)\n",
        "                pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "                img = Image.open(io.BytesIO(pix.tobytes()))\n",
        "                images.append(img)\n",
        "            return images\n",
        "        else:\n",
        "            print(f\"Unsupported file format: {os.path.basename(file_path)}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def process_gemini_file(file_name, folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    prompt = \"Extract: Student ID (number, must be 7 digits), all Outcomes (number: value), Total Marks (number).\"\n",
        "    row_dict = {}\n",
        "    try:\n",
        "        images = load_file(file_path)\n",
        "        if not images:\n",
        "            logger.warning(f\"No images loaded from {file_name}\")\n",
        "            return row_dict, file_name\n",
        "        for img in images:\n",
        "            response = generate_gemini_content([img, prompt])\n",
        "            generated_text = response.text.replace(\"*\", \"\").strip()\n",
        "            if not generated_text:\n",
        "                logger.warning(f\"Empty response from Gemini for {file_name}\")\n",
        "                return row_dict, file_name\n",
        "            # Extract Student ID\n",
        "            student_id_match = re.search(r'Student ID[:\\s]*(\\d+)', generated_text, re.IGNORECASE)\n",
        "            if student_id_match:\n",
        "                row_dict['Student ID'] = str(student_id_match.group(1))\n",
        "            # Extract Outcomes\n",
        "            outcome_matches = re.findall(r'Outcome\\s*(\\d+)[:\\s]*(\\d+)', generated_text, re.IGNORECASE)\n",
        "            for num, mark in outcome_matches:\n",
        "                row_dict[f'Outcome {num}'] = int(mark)\n",
        "            # Extract Total Marks\n",
        "            total_marks_match = re.search(r'Total Marks[:\\s]*(\\d+)', generated_text, re.IGNORECASE)\n",
        "            if total_marks_match:\n",
        "                row_dict['Total Marks'] = int(total_marks_match.group(1))\n",
        "            if not ('Student ID' in row_dict and any('Outcome' in k for k in row_dict) and 'Total Marks' in row_dict):\n",
        "                logger.warning(f\"Incomplete data extracted from {file_name}\")\n",
        "                return row_dict, file_name\n",
        "        return row_dict, None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {file_name} with Gemini: {str(e)}\")\n",
        "        return row_dict, file_name\n",
        "\n",
        "def process_llama_file(file_name, folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    row_dict = {}\n",
        "    try:\n",
        "        images = load_file(file_path)\n",
        "        if not images:\n",
        "            logger.warning(f\"No images loaded from {file_name}\")\n",
        "            return row_dict, file_name\n",
        "        for img in images:\n",
        "            if llama_model is None or llama_tokenizer is None:\n",
        "                logger.warning(f\"LLaMA model not initialized for {file_name}\")\n",
        "                return row_dict, file_name\n",
        "            prompt = \"Extract: Student ID (number, must be 7 digits), all Outcomes (number: value), Total Marks (number).\"\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                ]}\n",
        "            ]\n",
        "            input_text = llama_tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "            inputs = llama_tokenizer(\n",
        "                img,\n",
        "                input_text,\n",
        "                add_special_tokens=False,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(\"cuda\")\n",
        "            output_tokens = llama_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                use_cache=True,\n",
        "                temperature=0.0,\n",
        "                do_sample=False\n",
        "            )\n",
        "            generated_text = llama_tokenizer.decode(output_tokens[0], skip_special_tokens=True).strip()\n",
        "            generated_text = generated_text.replace(\"*\", \"\").replace(\"Course Performance:\", \"\")\n",
        "            # Extract Student ID\n",
        "            student_id_match = re.search(r'Student ID[:\\s]*(\\d+)', generated_text, re.IGNORECASE)\n",
        "            if student_id_match:\n",
        "                row_dict['Student ID'] = str(student_id_match.group(1))\n",
        "            # Extract Outcomes\n",
        "            outcome_matches = re.findall(r'Outcome\\s*(\\d+)[:\\s]*(\\d+)', generated_text, re.IGNORECASE)\n",
        "            for num, mark in outcome_matches:\n",
        "                row_dict[f'Outcome {num}'] = int(mark)\n",
        "            # Extract Total Marks\n",
        "            total_marks_match = re.search(r'Total Marks[:\\s]*(\\d+)', generated_text, re.IGNORECASE)\n",
        "            if total_marks_match:\n",
        "                row_dict['Total Marks'] = int(total_marks_match.group(1))\n",
        "            if not ('Student ID' in row_dict and any('Outcome' in k for k in row_dict) and 'Total Marks' in row_dict):\n",
        "                logger.warning(f\"Incomplete data extracted from {file_name}\")\n",
        "                return row_dict, file_name\n",
        "        return row_dict, None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {file_name} with LLaMA: {str(e)}\")\n",
        "        return row_dict, file_name\n",
        "\n",
        "def process_files(model_name, selected_files, selected_folder, batch_size=5, sleep_time=0.5):\n",
        "    folder_path = os.path.join(BASE_DIR, selected_folder)\n",
        "    list_rows = []\n",
        "    all_keys = set()\n",
        "    failed_files = []\n",
        "    total_files = len(selected_files)\n",
        "    processed_files = 0\n",
        "\n",
        "    def process_single_file(file_name):\n",
        "        return process_gemini_file(file_name, folder_path) if model_name == \"Gemini\" else process_llama_file(file_name, folder_path)\n",
        "\n",
        "    batched_files = [selected_files[i:i + batch_size] for i in range(0, len(selected_files), batch_size)]\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for batch in batched_files:\n",
        "            future_to_file = {executor.submit(process_single_file, file_name): file_name for file_name in batch}\n",
        "            for future in as_completed(future_to_file):\n",
        "                processed_files += 1\n",
        "                progress = processed_files / total_files\n",
        "                st.session_state.progress_bar.progress(min(progress, 1.0))\n",
        "                row_dict, failed_file = future.result()\n",
        "                if row_dict:\n",
        "                    all_keys.update(row_dict.keys())\n",
        "                    list_rows.append(row_dict)\n",
        "                if failed_file:\n",
        "                    failed_files.append(failed_file)\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "    if list_rows:\n",
        "        all_keys_list = list(all_keys)\n",
        "        student_id_key = 'Student ID'\n",
        "        total_marks_key = 'Total Marks'\n",
        "        outcome_keys = sorted([key for key in all_keys_list if key.startswith('Outcome')],\n",
        "                             key=lambda x: int(re.search(r'\\d+', x).group()))\n",
        "        other_keys = [key for key in all_keys_list if key not in [student_id_key, total_marks_key] and not key.startswith('Outcome')]\n",
        "        ordered_keys = [student_id_key] + outcome_keys + [total_marks_key] + other_keys\n",
        "        normalized_rows = [{key: row.get(key, None) for key in ordered_keys} for row in list_rows]\n",
        "        df = pd.DataFrame(normalized_rows, columns=ordered_keys)\n",
        "        df['Student ID'] = df['Student ID'].astype(str)\n",
        "        logger.info(f\"DataFrame created with {len(df)} rows and columns: {ordered_keys}\")\n",
        "        return df, failed_files\n",
        "    return pd.DataFrame(), failed_files\n",
        "\n",
        "# --- MODIFIED DATABASE SECTION STARTS HERE ---\n",
        "\n",
        "def init_db():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    cursor = conn.cursor()\n",
        "    # Create normalized tables\n",
        "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS students (\n",
        "                    student_id TEXT PRIMARY KEY,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "                )\"\"\")\n",
        "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS outcomes (\n",
        "                    outcome_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    student_id TEXT,\n",
        "                    outcome_number INTEGER,\n",
        "                    score INTEGER,\n",
        "                    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    UNIQUE(student_id, outcome_number),\n",
        "                    FOREIGN KEY(student_id) REFERENCES students(student_id)\n",
        "                )\"\"\")\n",
        "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS total_marks (\n",
        "                    mark_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    student_id TEXT UNIQUE,\n",
        "                    total INTEGER,\n",
        "                    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    FOREIGN KEY(student_id) REFERENCES students(student_id)\n",
        "                )\"\"\")\n",
        "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {LOG_TABLE} (\n",
        "                    log_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    timestamp TIMESTAMP,\n",
        "                    action TEXT,\n",
        "                    details TEXT\n",
        "                )\"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_db()\n",
        "\n",
        "def log_action(action, details):\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    cursor = conn.cursor()\n",
        "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    cursor.execute(f\"INSERT INTO {LOG_TABLE} (timestamp, action, details) VALUES (?, ?, ?)\", (timestamp, action, details))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def save_to_sqlite(df):\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    try:\n",
        "        for _, row in df.iterrows():\n",
        "            student_id = str(row['Student ID']).strip()\n",
        "            # Insert or ignore student\n",
        "            conn.execute(\"INSERT OR IGNORE INTO students (student_id) VALUES (?)\", (student_id,))\n",
        "            # Insert outcomes with conflict handling\n",
        "            outcomes = [(k.replace('Outcome ', ''), v) for k, v in row.items()\n",
        "                       if k.startswith('Outcome') and pd.notnull(v)]\n",
        "            for outcome_num, score in outcomes:\n",
        "                conn.execute(\"\"\"INSERT INTO outcomes (student_id, outcome_number, score)\n",
        "                              VALUES (?, ?, ?)\n",
        "                              ON CONFLICT(student_id, outcome_number) DO UPDATE SET\n",
        "                              score = excluded.score,\n",
        "                              recorded_at = CURRENT_TIMESTAMP\"\"\",\n",
        "                           (student_id, outcome_num, score))\n",
        "            # Insert total marks\n",
        "            if pd.notnull(row.get('Total Marks')):\n",
        "                conn.execute(\"\"\"INSERT INTO total_marks (student_id, total)\n",
        "                              VALUES (?, ?)\n",
        "                              ON CONFLICT(student_id) DO UPDATE SET\n",
        "                              total = excluded.total,\n",
        "                              recorded_at = CURRENT_TIMESTAMP\"\"\",\n",
        "                           (student_id, row['Total Marks']))\n",
        "        conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        conn.rollback()\n",
        "        logger.error(f\"Database error: {str(e)}\")\n",
        "        st.error(\"Failed to save data to database. Check logs for details.\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "\n",
        "\n",
        "def load_from_sqlite():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    try:\n",
        "        # Load all outcomes dynamically\n",
        "        outcomes_df = pd.read_sql_query(\"\"\"\n",
        "            SELECT student_id, outcome_number, score\n",
        "            FROM outcomes\n",
        "        \"\"\", conn)\n",
        "\n",
        "        if outcomes_df.empty:\n",
        "            outcomes_pivot = pd.DataFrame()\n",
        "        else:\n",
        "            outcomes_df['Outcome'] = 'Outcome ' + outcomes_df['outcome_number'].astype(str)\n",
        "            outcomes_pivot = outcomes_df.pivot(index='student_id', columns='Outcome', values='score').reset_index()\n",
        "\n",
        "        # Load total marks\n",
        "        total_marks_df = pd.read_sql_query(\"\"\"\n",
        "            SELECT student_id, total AS 'Total Marks'\n",
        "            FROM total_marks\n",
        "        \"\"\", conn)\n",
        "\n",
        "        # Merge both\n",
        "        df = pd.merge(outcomes_pivot, total_marks_df, on='student_id', how='outer')\n",
        "        df = df.rename(columns={'student_id': 'Student ID'})\n",
        "        return df\n",
        "    except sqlite3.Error as e:\n",
        "        logger.error(f\"Database error: {str(e)}\")\n",
        "        return pd.DataFrame()\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "# Adding a function to view the log records\n",
        "def view_logs():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    df = pd.read_sql_query(f\"SELECT * FROM {LOG_TABLE}\", conn)\n",
        "    conn.close()\n",
        "    return df\n",
        "\n",
        "def merge_student_ids(extracted_df, student_ids_df):\n",
        "  # Merge extracted data with student IDs from Excel using fuzzy matching.\n",
        "  # Ensures compatibility of data types and matches IDs based on similarity.\n",
        "  try:\n",
        "        extracted_df.columns = extracted_df.columns.str.strip()\n",
        "        student_ids_df.columns = student_ids_df.columns.str.strip()\n",
        "        if 'Student ID' not in student_ids_df.columns:\n",
        "            raise ValueError(\"Student IDs file must contain a 'Student ID' column.\")\n",
        "        if 'Student ID' not in extracted_df.columns:\n",
        "            raise ValueError(\"Extracted data must contain a 'Student ID' column.\")\n",
        "        student_ids_df['Student ID'] = student_ids_df['Student ID'].astype(str).str.strip()\n",
        "        extracted_df['Student ID'] = extracted_df['Student ID'].astype(str).str.strip()\n",
        "        logger.info(f\"student_ids_df['Student ID'] dtype: {student_ids_df['Student ID'].dtype}\")\n",
        "        logger.info(f\"extracted_df['Student ID'] dtype: {extracted_df['Student ID'].dtype}\")\n",
        "        matches = []\n",
        "        threshold = 80\n",
        "        for student_id in student_ids_df['Student ID']:\n",
        "            best_match = None\n",
        "            highest_score = 0\n",
        "            for extracted_id in extracted_df['Student ID']:\n",
        "                score = fuzz.ratio(student_id, extracted_id)\n",
        "                if score > highest_score and score >= threshold:\n",
        "                    highest_score = score\n",
        "                    best_match = extracted_id\n",
        "            matches.append((student_id, best_match))\n",
        "        match_df = pd.DataFrame(matches, columns=['Student ID', 'Matched Student ID'])\n",
        "        merged_df = pd.merge(student_ids_df, match_df, on='Student ID', how='left')\n",
        "        final_df = pd.merge(merged_df, extracted_df, left_on='Matched Student ID', right_on='Student ID', how='left')\n",
        "        final_df = final_df.drop(columns=['Matched Student ID', 'Student ID_y']).rename(columns={'Student ID_x': 'Student ID'})\n",
        "        merged_df = final_df.fillna('Not Found')\n",
        "        return merged_df\n",
        "\n",
        "  except Exception as e:\n",
        "        logger.error(f\"Error during merging student IDs: {str(e)}\")\n",
        "        st.error(f\"Error during merging student IDs: {str(e)}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# --- MAIN APP ---\n",
        "\n",
        "def main():\n",
        "    st.title(\"Student Results Extractor\")\n",
        "    st.write(\"Extract Student IDs, Outcomes, and Total Marks from images or PDFs.\")\n",
        "\n",
        "    # View logs section\n",
        "    st.subheader(\"ðŸ“‹ View Action Logs\")\n",
        "    if st.button(\"View Logs\"):\n",
        "        logs_df = view_logs()\n",
        "        if not logs_df.empty:\n",
        "            st.dataframe(logs_df)\n",
        "        else:\n",
        "            st.info(\"No logs available.\")\n",
        "\n",
        "    if not os.path.exists(BASE_DIR):\n",
        "        st.error(\"Google Drive not mounted. Run in Colab: `from google.colab import drive; drive.mount('/content/drive')`\")\n",
        "        return\n",
        "\n",
        "    model_choice = st.selectbox(\"Select Model:\", [\"Gemini\", \"LLaMA\"])\n",
        "\n",
        "    available_folders = [f for f in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, f))]\n",
        "    available_folders.sort()\n",
        "    selected_folder = st.selectbox(\"Select Folder from MyDrive:\", available_folders)\n",
        "\n",
        "    if selected_folder:\n",
        "        folder_path = os.path.join(BASE_DIR, selected_folder)\n",
        "        available_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.pdf'))]\n",
        "        available_files.sort()\n",
        "        if not available_files:\n",
        "            st.warning(f\"No JPG or PDF files found in '{selected_folder}'.\")\n",
        "            return\n",
        "\n",
        "        select_all = st.checkbox(\"Select All Files\", value=False)\n",
        "        selected_files = st.multiselect(\n",
        "            \"Select Files:\",\n",
        "            available_files,\n",
        "            default=available_files if select_all else []\n",
        "        )\n",
        "\n",
        "        student_ids_file = st.file_uploader(\"Upload Student IDs Excel File (for merging)\", type=['xlsx'],\n",
        "                                           help=\"Excel file with Student IDs to merge with extracted data.\")\n",
        "\n",
        "        if st.button(\"Process Files\"):\n",
        "            if not selected_files:\n",
        "                st.warning(\"Please select at least one file.\")\n",
        "            else:\n",
        "                with st.spinner(f\"Processing {len(selected_files)} files with {model_choice}...\"):\n",
        "                    st.session_state.progress_bar = st.progress(0.0)\n",
        "                    start_time = time.time()\n",
        "                    df, failed_files = process_files(model_choice, selected_files, selected_folder)\n",
        "                    end_time = time.time()\n",
        "\n",
        "                if not df.empty:\n",
        "                    st.success(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
        "                    st.write(\"Extracted Results:\")\n",
        "                    st.dataframe(df)\n",
        "                    save_to_sqlite(df)\n",
        "                    st.success(\"Saved to SQLite database.\")\n",
        "                    csv_path = \"student_results.csv\"\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    st.download_button(\n",
        "                        label=\"Download Results as CSV\",\n",
        "                        data=open(csv_path, \"rb\").read(),\n",
        "                        file_name=\"student_results.csv\",\n",
        "                        mime=\"text/csv\"\n",
        "                    )\n",
        "\n",
        "                if student_ids_file and not df.empty:\n",
        "                    st.subheader(\"Merged Results with Student IDs\")\n",
        "                    student_ids_df = pd.read_excel(student_ids_file)\n",
        "                    merged_df = merge_student_ids(df, student_ids_df)\n",
        "                    if not merged_df.empty:\n",
        "                        st.dataframe(merged_df)\n",
        "                        merged_csv_path = \"merged_student_results.csv\"\n",
        "                        merged_df.to_csv(merged_csv_path, index=False)\n",
        "                        with open(merged_csv_path, \"rb\") as f:\n",
        "                            st.download_button(\n",
        "                                label=\"Download Merged Results as CSV\",\n",
        "                                data=f,\n",
        "                                file_name=\"merged_student_results.csv\",\n",
        "                                mime=\"text/csv\"\n",
        "                            )\n",
        "                    else:\n",
        "                        st.error(\"No data extracted from files.\")\n",
        "\n",
        "                if failed_files:\n",
        "                    st.warning(\"Failed to process or extract data from:\")\n",
        "                    for file in failed_files:\n",
        "                        st.write(f\"- {file}\")\n",
        "                    st.write(\"Check '/content/processing_log.log' for details.\")\n",
        "                else:\n",
        "                    st.success(\"All files processed successfully.\")\n",
        "\n",
        "\n",
        "    st.subheader(\"ðŸ“Š View Stored Results\")\n",
        "\n",
        "    if st.button(\"Load from Database\"):\n",
        "        stored_data = load_from_sqlite()\n",
        "        if not stored_data.empty:\n",
        "            st.session_state[\"loaded_data\"] = stored_data\n",
        "            st.success(\"Data loaded successfully.\")\n",
        "        else:\n",
        "            st.warning(\"No data in database.\")\n",
        "\n",
        "    # Display after load\n",
        "    if \"loaded_data\" in st.session_state:\n",
        "        st.write(\"### Stored Results\")\n",
        "        st.dataframe(st.session_state[\"loaded_data\"], use_container_width=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9ggOHscD6NH",
        "outputId": "74fc0334-2898-456a-a075-21de3ea5bdd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Streamlit app is running at: NgrokTunnel: \"https://900d-34-171-242-210.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "\n",
        "# Retrieve ngrok token from Colab Secrets\n",
        "ngrok_token = userdata.get('NGROK_TOKEN')\n",
        "if not ngrok_token:\n",
        "    raise ValueError(\"ngrok token not found. Please make sure it's added under Colab > Settings > Secrets.\")\n",
        "\n",
        "# Set ngrok auth token\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Mount Google Drive (optional if you are using files from Drive)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Kill any previous ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Launch the Streamlit app\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '8501'])\n",
        "time.sleep(5)  # Wait for Streamlit to start\n",
        "\n",
        "# Create a public URL using ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is running at: {public_url}\")\n",
        "\n",
        "# Keep the app alive\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping Streamlit and ngrok\")\n",
        "    process.terminate()\n",
        "    ngrok.kill()\n"
      ]
    }
  ]
}